{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg4XdqdRcB8a"
      },
      "source": [
        "# Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYWZIHXZEI6s"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "from xgboost import XGBClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.over_sampling import SMOTENC\n",
        "from imblearn.pipeline import Pipeline\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials, space_eval\n",
        "import shap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0K4tFDq1fJ7w",
        "outputId": "31e6a57c-7af6-4c9c-9350-59eb4580912c"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MttqObelcKKV"
      },
      "source": [
        "# Reading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-XjwHDCFMF7",
        "outputId": "c47e45ab-9957-4ba0-a879-818fafc3ef28"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/Sleep_health_and_lifestyle_dataset.csv'\n",
        "\n",
        "if not os.path.exists(file_path):\n",
        "    raise FileNotFoundError('파일을 찾을 수 없습니다.')\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "print(df.shape)\n",
        "print(df.columns.tolist())\n",
        "print(df.head())\n",
        "print(df.info())\n",
        "print(df.describe())\n",
        "print(df.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2sClUbMckdt"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "CaN-mTu3GW4z",
        "outputId": "f5f1a035-5403-4d48-9075-7a188b3c5fba"
      },
      "outputs": [],
      "source": [
        "print(df.select_dtypes(include=np.number).columns.tolist())\n",
        "\n",
        "for col in df.select_dtypes(include=np.number).columns.tolist():\n",
        "    plt.figure(figsize=(6,3))\n",
        "    sns.histplot(df[col].dropna(), kde=True)\n",
        "    plt.title(f'{col} Distribution')\n",
        "    plt.show()\n",
        "\n",
        "print(df.select_dtypes(exclude=np.number).columns.tolist())\n",
        "\n",
        "for col in df.select_dtypes(exclude=np.number).columns.tolist():\n",
        "    plt.figure(figsize=(5,3))\n",
        "    df[col].value_counts(dropna=False).plot(kind='bar')\n",
        "    plt.title(f'{col} Distribution')\n",
        "    plt.show()\n",
        "\n",
        "sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True, cmap='coolwarm')\n",
        "plt.title(\"Heatmap\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD2IRf8qctVy"
      },
      "source": [
        "# Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX95ZDLiJKp5",
        "outputId": "be13d6e4-676d-4333-aa8c-dd058bb6e437"
      },
      "outputs": [],
      "source": [
        "target_col = 'Sleep Disorder'\n",
        "\n",
        "if target_col not in df.columns:\n",
        "    raise KeyError('Sleep Disorder가 없습니다.')\n",
        "\n",
        "y_raw = df[target_col].fillna('Normal').astype(str)\n",
        "le_target = LabelEncoder()\n",
        "y = le_target.fit_transform(y_raw)\n",
        "\n",
        "print('타겟 클래스:', le_target.classes_)\n",
        "print('레이블 매핑:', dict(zip(le_target.classes_, le_target.transform(le_target.classes_))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8F1VJeiKFtv"
      },
      "outputs": [],
      "source": [
        "X = df.copy()\n",
        "\n",
        "if 'Person ID' not in X.columns:\n",
        "   raise KeyError('Person ID가 없습니다.')\n",
        "\n",
        "X.drop(columns=['Sleep Disorder', 'Person ID'], inplace=True)\n",
        "\n",
        "bp = [c for c in X.columns if 'Blood Pressure' in c]\n",
        "\n",
        "# Blood Pressure을 Systolic/Diastolic로 분리\n",
        "if bp:\n",
        "  bp_col = bp[0]\n",
        "  bp = X[bp_col].astype(str).str.extract(r'(?P<Systolic>\\d{2,3})\\D+(?P<Diastolic>\\d{2,3})')\n",
        "  X['Systolic'] = pd.to_numeric(bp['Systolic'], errors='coerce')\n",
        "  X['Diastolic'] = pd.to_numeric(bp['Diastolic'], errors='coerce')\n",
        "  X.drop(columns=[bp_col], inplace=True)\n",
        "else:\n",
        "  X['Systolic'] = np.nan\n",
        "  X['Diastolic'] = np.nan\n",
        "\n",
        "# 혈압 상태 카테고리 분류 함수\n",
        "def bp_category(sys, dia):\n",
        "    if pd.isna(sys) or pd.isna(dia): return 'Unknown'\n",
        "    if sys >= 140 or dia >= 90: return 'High'\n",
        "    if (130 <= sys <= 139) or (80 <= dia <= 89): return 'Borderline'\n",
        "    return 'Normal'\n",
        "\n",
        "# 혈압 상태 카테고리 분류 적용\n",
        "X['BP Category'] = X.apply(lambda r: bp_category(r['Systolic'], r['Diastolic']), axis=1)\n",
        "X.drop(columns=['Systolic', 'Diastolic'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bg_UBxJGVPvn"
      },
      "outputs": [],
      "source": [
        "cat_cols = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "# 레이블 인코딩을 적용\n",
        "le_dict = {}\n",
        "\n",
        "for col in cat_cols:\n",
        "    le = LabelEncoder()\n",
        "    X[col] = le.fit_transform(X[col].astype(str))\n",
        "    le_dict[col] = le # 역변환을 위해 인코더 저장"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AOae0WAL0B9",
        "outputId": "c3d4f331-b2e5-4375-a603-1fba853ae6d4"
      },
      "outputs": [],
      "source": [
        "# 학습/테스트 데이터 분할, 클래스 비율 유지\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# 테스트 데이터에서 검증 데이터 추가 분할(과적합 방지용)\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
        ")\n",
        "\n",
        "print(f'학습 데이터: {X_tr.shape}, 검증 데이터: {X_val.shape}, 테스트 데이터: {X_test.shape}')\n",
        "\n",
        "categorical_features = [X.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "# SMOTENC 설정\n",
        "smote_nc = SMOTENC(\n",
        "    categorical_features=categorical_features,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 학습 데이터를 대상으로 클래스 불균형 처리\n",
        "# 연속형 + 범주형 데이터 혼합 환경에서 오버샘플링을 수행\n",
        "X_tr_res, y_tr_res = smote_nc.fit_resample(X_tr, y_tr)\n",
        "\n",
        "print(f'학습 데이터(SMOTE 적용 후): {X_tr_res.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaWwWU9Hc1Qx"
      },
      "source": [
        "# Data modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktqrbNs6L388"
      },
      "outputs": [],
      "source": [
        "results = {} # 모델별 하이퍼파라미터 실험 결과 저장\n",
        "\n",
        "xgb_baseline = XGBClassifier(\n",
        "    n_estimators=150,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    objective='multi:softprob',\n",
        "    num_class=3\n",
        ")\n",
        "\n",
        "xgb_baseline.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)\n",
        "pred_xgb_baseline = xgb_baseline.predict(X_test)\n",
        "\n",
        "results['XGB_Baseline'] = {\n",
        "    'model': xgb_baseline,\n",
        "    'pred': pred_xgb_baseline,\n",
        "    'pred_proba': xgb_baseline.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_baseline),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_baseline, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_baseline, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_baseline, average='macro')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_baseline_result = xgb_baseline.evals_result_\n",
        "epochs = len(xgb_baseline_result['validation_0']['mlogloss'])\n",
        "\n",
        "plt.title('Learning Curve')\n",
        "plt.plot(range(epochs), xgb_baseline_result['validation_0']['mlogloss'], label='Train')\n",
        "plt.plot(range(epochs), xgb_baseline_result['validation_1']['mlogloss'], label='Valid')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('mlogloss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_tuned = XGBClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.01,\n",
        "    max_depth=3,\n",
        "    min_child_weight=3,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.65,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    early_stopping_rounds=50,\n",
        "    objective='multi:softprob',\n",
        "    num_class=3\n",
        ")\n",
        "\n",
        "xgb_tuned.fit(X_tr, y_tr, eval_set=[(X_tr, y_tr), (X_val, y_val)], verbose=False)\n",
        "pred_xgb_tuned = xgb_tuned.predict(X_test)\n",
        "\n",
        "results['XGB_Tuned'] = {\n",
        "    'model': xgb_tuned,\n",
        "    'pred': pred_xgb_tuned,\n",
        "    'pred_proba': xgb_tuned.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_tuned),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_tuned, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_tuned, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_tuned, average='macro')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_tuned_result = xgb_tuned.evals_result_\n",
        "epochs = len(xgb_tuned_result['validation_0']['mlogloss'])\n",
        "\n",
        "plt.title('Learning Curve')\n",
        "plt.plot(range(epochs), xgb_tuned_result['validation_0']['mlogloss'], label='Train')\n",
        "plt.plot(range(epochs), xgb_tuned_result['validation_1']['mlogloss'], label='Valid')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('mlogloss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RandomizedSearchCV를 위한 하이퍼파라미터 탐색 범위 설정\n",
        "param_dist = {\n",
        "    'n_estimators': [300, 500, 700],\n",
        "    'learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
        "    'max_depth': [2, 3, 4, 5],\n",
        "    'min_child_weight': [2, 3, 5, 7],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "}\n",
        "\n",
        "xgb_random_search = RandomizedSearchCV(\n",
        "    XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        objective='multi:softprob',\n",
        "        num_class=3\n",
        "    ),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    random_state=42,\n",
        ")\n",
        "\n",
        "xgb_random_search.fit(X_train, y_train)\n",
        "pred_xgb_random_search = xgb_random_search.predict(X_test)\n",
        "\n",
        "results['XGB_RandomSearch'] = {\n",
        "    'model': xgb_random_search,\n",
        "    'pred': pred_xgb_random_search,\n",
        "    'pred_proba': xgb_random_search.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_random_search),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_random_search, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_random_search, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_random_search, average='macro')\n",
        "}\n",
        "\n",
        "print(f'최적 파라미터: {xgb_random_search.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GridSearchCV를 위한 하이퍼파라미터 탐색 범위 설정\n",
        "param_grid = {\n",
        "    'n_estimators': [300, 500],\n",
        "    'learning_rate': [0.01, 0.05],\n",
        "    'max_depth': [2, 3, 4],\n",
        "    'min_child_weight': [3, 5],\n",
        "    'subsample': [0.7, 0.8],\n",
        "    'colsample_bytree': [0.7, 0.8],\n",
        "}\n",
        "\n",
        "xgb_grid_search = GridSearchCV(\n",
        "    XGBClassifier(\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        objective='multi:softprob',\n",
        "        num_class=3\n",
        "    ),\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        ")\n",
        "\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "pred_xgb_grid_search = xgb_grid_search.predict(X_test)\n",
        "\n",
        "results['XGB_GridSearch'] = {\n",
        "    'model': xgb_grid_search,\n",
        "    'pred': pred_xgb_grid_search,\n",
        "    'pred_proba': xgb_grid_search.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_grid_search),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_grid_search, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_grid_search, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_grid_search, average='macro')\n",
        "}\n",
        "\n",
        "\n",
        "print(f'최적 파라미터: {xgb_grid_search.best_params_}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperopt를 위한 하이퍼파라미터 탐색 범위 설정\n",
        "# 베이지안 최적화 기반 탐색\n",
        "hyperopt_space = {\n",
        "    'n_estimators': hp.quniform('n_estimators', 250, 500, 50),\n",
        "    'learning_rate': hp.uniform('learning_rate', 0.03, 0.15),\n",
        "    'max_depth': hp.quniform('max_depth', 2, 4, 1),\n",
        "    'min_child_weight': hp.quniform('min_child_weight', 4, 8, 1),\n",
        "    'subsample': hp.uniform('subsample', 0.6, 0.8),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 0.75),\n",
        "}\n",
        "\n",
        "# Hyperopt를 위한 하이퍼파라미터 탐색 목적 함수 정의\n",
        "def objective_hyperopt(params):\n",
        "    params['n_estimators'] = int(params['n_estimators'])\n",
        "    params['max_depth'] = int(params['max_depth'])\n",
        "    params['min_child_weight'] = int(params['min_child_weight'])\n",
        "    model = XGBClassifier(\n",
        "        **params,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        objective='multi:softprob',\n",
        "        num_class=3\n",
        "    )\n",
        "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1_macro').mean()\n",
        "    return {'loss': -score, 'status': STATUS_OK}\n",
        "\n",
        "trials = Trials()\n",
        "best_params_hyperopt = fmin(\n",
        "    fn=objective_hyperopt,\n",
        "    space=hyperopt_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials,\n",
        "    rstate=np.random.default_rng(42),\n",
        ")\n",
        "\n",
        "best_params_hyperopt['n_estimators'] = int(best_params_hyperopt['n_estimators'])\n",
        "best_params_hyperopt['max_depth'] = int(best_params_hyperopt['max_depth'])\n",
        "best_params_hyperopt['min_child_weight'] = int(best_params_hyperopt['min_child_weight'])\n",
        "\n",
        "xgb_hyperopt = XGBClassifier(\n",
        "    **best_params_hyperopt,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        "    early_stopping_rounds=30,\n",
        ")\n",
        "\n",
        "xgb_hyperopt.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "pred_xgb_hyperopt = xgb_hyperopt.predict(X_test)\n",
        "\n",
        "results['XGB_Hyperopt'] = {\n",
        "    'model': xgb_hyperopt,\n",
        "    'pred': pred_xgb_hyperopt,\n",
        "    'pred_proba': xgb_hyperopt.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_hyperopt),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_hyperopt, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_hyperopt, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_hyperopt, average='macro')\n",
        "}\n",
        "\n",
        "best_params = space_eval(hyperopt_space, best_params_hyperopt)\n",
        "print(f'최적 파라미터 {best_params}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Optuna를 위한 하이퍼파라미터 탐색 목적 함수 정의\n",
        "# 베이지안 최적화 기반 탐색\n",
        "def objective_optuna(trial):\n",
        "    params = {\n",
        "        'n_estimators': trial.suggest_int('n_estimators', 250, 500, step=50),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 0.03, 0.15),\n",
        "        'max_depth': trial.suggest_int('max_depth', 2, 4),\n",
        "        'min_child_weight': trial.suggest_int('min_child_weight', 4, 8),\n",
        "        'subsample': trial.suggest_float('subsample', 0.6, 0.8),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.75),\n",
        "        'random_state': 42,\n",
        "        'eval_metric': 'mlogloss',\n",
        "        'objective':'multi:softprob',\n",
        "        'num_class': 3\n",
        "    }\n",
        "\n",
        "    xgb_optuna = XGBClassifier(**params)\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    scores = cross_val_score(xgb_optuna, X_train, y_train, cv=cv, scoring='f1_macro').mean()\n",
        "\n",
        "    return scores\n",
        "\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective_optuna, n_trials=50)\n",
        "\n",
        "xgb_optuna = XGBClassifier(\n",
        "    **study.best_params,\n",
        "    random_state=42,\n",
        "    eval_metric='mlogloss',\n",
        "    early_stopping_rounds=30,\n",
        "    objective='multi:softprob',\n",
        "    num_class=3,\n",
        ")\n",
        "\n",
        "xgb_optuna.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "pred_xgb_optuna = xgb_optuna.predict(X_test)\n",
        "\n",
        "results['XGB_Optuna'] = {\n",
        "    'model': xgb_optuna,\n",
        "    'pred': pred_xgb_optuna,\n",
        "    'pred_proba': xgb_optuna.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_optuna),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_optuna, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_optuna, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_optuna, average='macro')\n",
        "}\n",
        "\n",
        "print(f'최적 파라미터: {study.best_params}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_baseline_smote = Pipeline([\n",
        "    ('smote', smote_nc),\n",
        "    ('model', XGBClassifier(\n",
        "        n_estimators=150,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=5,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        objective='multi:softprob',\n",
        "        num_class=3\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_baseline_smote.fit(X_tr, y_tr)\n",
        "pred_xgb_baseline = xgb_baseline_smote.predict(X_test)\n",
        "\n",
        "\n",
        "results['XGB_Baseline_SMOTE'] = {\n",
        "    'model': xgb_baseline_smote,\n",
        "    'pred': pred_xgb_baseline,\n",
        "    'pred_proba': xgb_baseline_smote.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_baseline),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_baseline, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_baseline, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_baseline, average='macro')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_tuned_smote = Pipeline([\n",
        "    ('smote', smote_nc),\n",
        "    ('model', XGBClassifier(\n",
        "        n_estimators=500,\n",
        "        learning_rate=0.08,\n",
        "        max_depth=3,\n",
        "        min_child_weight=7,\n",
        "        subsample=0.7,\n",
        "        colsample_bytree=0.65,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        objective='multi:softprob',\n",
        "        num_class=3\n",
        "    ))\n",
        "])\n",
        "\n",
        "xgb_tuned_smote.fit(X_tr, y_tr)\n",
        "pred_xgb_tuned_smote = xgb_tuned_smote.predict(X_test)\n",
        "\n",
        "results['XGB_Tuned_SMOTE'] = {\n",
        "    'model': xgb_tuned_smote,\n",
        "    'pred': pred_xgb_tuned_smote,\n",
        "    'pred_proba': xgb_tuned_smote.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_tuned_smote),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_tuned_smote, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_tuned_smote, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_tuned_smote, average='macro')\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_dist = {\n",
        "    'model__n_estimators': [300, 500, 700],\n",
        "    'model__learning_rate': [0.01, 0.03, 0.05, 0.1],\n",
        "    'model__max_depth': [2, 3, 4, 5],\n",
        "    'model__min_child_weight': [2, 3, 5, 7],\n",
        "    'model__subsample': [0.7, 0.8, 0.9],\n",
        "    'model__colsample_bytree': [0.7, 0.8, 0.9],\n",
        "}\n",
        "\n",
        "xgb_random_search_smote = RandomizedSearchCV(\n",
        "    Pipeline([\n",
        "        ('smote', smote_nc),\n",
        "        ('model', XGBClassifier(\n",
        "            eval_metric='mlogloss',\n",
        "            objective='multi:softprob',\n",
        "            num_class=3,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=50,\n",
        "    scoring='f1_macro',\n",
        "    cv=5,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "xgb_random_search_smote.fit(X_train, y_train)\n",
        "pred_xgb_random_smote = xgb_random_search_smote.predict(X_test)\n",
        "\n",
        "results['XGB_RandomSearch_SMOTE'] = {\n",
        "    'model': xgb_random_search_smote,\n",
        "    'pred': pred_xgb_random_smote,\n",
        "    'pred_proba': xgb_random_search_smote.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_random_smote),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_random_smote, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_random_smote, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_random_smote, average='macro')\n",
        "}\n",
        "\n",
        "print(f'최적 파라미터: {xgb_random_search_smote.best_params_}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "param_grid = {\n",
        "    'model__n_estimators': [300, 500],\n",
        "    'model__learning_rate': [0.01, 0.05],\n",
        "    'model__max_depth': [2, 3, 4],\n",
        "    'model__min_child_weight': [3, 5],\n",
        "    'model__subsample': [0.7, 0.8],\n",
        "    'model__colsample_bytree': [0.7, 0.8]\n",
        "}\n",
        "\n",
        "\n",
        "xgb_grid_smote = GridSearchCV(\n",
        "    Pipeline([\n",
        "        ('smote', smote_nc),\n",
        "        ('model', XGBClassifier(\n",
        "            eval_metric='mlogloss',\n",
        "            objective='multi:softprob',\n",
        "            num_class=3,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "    param_grid=param_grid,\n",
        "    scoring='f1_macro',\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "xgb_grid_smote.fit(X_train, y_train)\n",
        "pred_xgb_grid_smote = xgb_grid_smote.predict(X_test)\n",
        "\n",
        "results['XGB_GridSearch_SMOTE'] = {\n",
        "    'model': xgb_grid_smote,\n",
        "    'pred': pred_xgb_grid_smote,\n",
        "    'pred_proba': xgb_grid_smote.predict_proba(X_test),\n",
        "    'accuracy': accuracy_score(y_test, pred_xgb_grid_smote),\n",
        "    'f1_macro': f1_score(y_test, pred_xgb_grid_smote, average='macro'),\n",
        "    'precision_macro': precision_score(y_test, pred_xgb_grid_smote, average='macro'),\n",
        "    'recall_macro': recall_score(y_test, pred_xgb_grid_smote, average='macro')\n",
        "}\n",
        "\n",
        "\n",
        "print(f'최적 파라미터: {xgb_grid_smote.best_params_}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImH8VNxddzws"
      },
      "source": [
        "# Models evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sJtBsHucMNi5",
        "outputId": "009f338a-537a-4870-fd10-dd25ce602151"
      },
      "outputs": [],
      "source": [
        "results_df = pd.DataFrame({\n",
        "    'Model': list(results.keys()),\n",
        "    'Accuracy': [v['accuracy'] for v in results.values()],\n",
        "    'F1_Macro': [v['f1_macro'] for v in results.values()],\n",
        "    'Recall_Macro': [v['recall_macro'] for v in results.values()],\n",
        "    'Precision_Macro': [v['precision_macro'] for v in results.values()],\n",
        "})\n",
        "\n",
        "w_f1 = 0.5 # 불균형 데이터 대응을 위해 가중치를 가장 높게 설정\n",
        "w_recall = 0.4\n",
        "w_precision = 0.1\n",
        "\n",
        "results_df['Weight_Score'] = (\n",
        "    results_df['F1_Macro'] * w_f1 +\n",
        "    results_df['Recall_Macro'] * w_recall +\n",
        "    results_df['Precision_Macro'] * w_precision\n",
        ")\n",
        "\n",
        "results_df = results_df.sort_values('Weight_Score', ascending=False)\n",
        "print(results_df)\n",
        "\n",
        "for metric in ['Weight_Score', 'F1_Macro', 'Recall_Macro', 'Precision_Macro', 'Accuracy']:\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.barplot(x=metric, y='Model', data=results_df)\n",
        "    plt.title(f'Model Comparison: {metric}')\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7O4ff8W6MjRn",
        "outputId": "4cbda0dc-e363-4bd2-83af-e9f28a063f35"
      },
      "outputs": [],
      "source": [
        "best_model_name = results_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "best_pred = results[best_model_name]['pred']\n",
        "\n",
        "print(f'Best Model: {best_model_name}')\n",
        "\n",
        "# RandomizedSearchCV이면 best_estimator_ 사용\n",
        "if hasattr(best_model, 'best_estimator_'):\n",
        "    final_model = best_model.best_estimator_\n",
        "else:\n",
        "    final_model = best_model\n",
        "\n",
        "print('\\nClassification report:')\n",
        "print(classification_report(y_test, best_pred, target_names=le_target.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, best_pred)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.heatmap(cm, annot=True, fmt='d', xticklabels=le_target.classes_, yticklabels=le_target.classes_, cmap='Blues')\n",
        "plt.xlabel('Predicted');\n",
        "plt.ylabel('True');\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "importances = pd.Series(final_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print('Feature Importances:\\n', importances)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "importances.plot(kind='bar')\n",
        "plt.title('Feature Importances (All Features)')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
